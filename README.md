**ðŸ¦™ LLaMA 3.1 (Quantized - q4\_k\_m) - Local CPU-Optimized Model**

This is a quantized version of **LLaMA 3.1**, saved in the `q4_k_m` format using the **GGUF** specification for efficient local inference. The quantization reduces the model size significantly while retaining strong performance, making it ideal for use on laptops, desktops, and low-resource environments without a GPU.

### ðŸ”§ Model Details:

* **Base Model**: LLaMA 3.1
* **Quantization**: `q4_k_m` (4-bit, optimized)
* **Format**: GGUF (GPT Graph Universal Format)
* **Use Case**: CPU-based inference using llama.cpp, Ollama, text-generation-webui, etc.

### ðŸš€ Highlights:

* Small footprint, runs fast on CPU
* Good balance of speed and accuracy
* Compatible with popular local inference tools
* Suitable for chatbots, prompt testing, local assistants, and embedded apps
